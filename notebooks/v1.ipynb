{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/renatoboemer/code/developer/luthor/data/Memo 2  - Crypto assets disposal - FINISHED .docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from docx import Document\n",
    "\n",
    "def read_file(file_path):\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Get the file extension\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    # Handle .txt files\n",
    "    if file_extension.lower() == '.txt':\n",
    "        return read_txt(file_path)\n",
    "\n",
    "    # Handle .docx files\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        return read_docx(file_path)\n",
    "\n",
    "    # Handle .pdf files\n",
    "    elif file_extension.lower() == '.pdf':\n",
    "        return read_pdf(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_docx(file_path):\n",
    "    document = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in document.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    document = fitz.open(file_path)\n",
    "    all_text = []\n",
    "    for page in document:\n",
    "        text = page.get_text()\n",
    "        all_text.append(text)\n",
    "    return '\\n'.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "I would like to seek expert advice regarding our company's upcoming transaction involving the disposal of cryptocurrency assets. Specifically, we are planning to sell a substantial amount of Ethereum (ETH) and convert it into stablecoins due to market volatility. Please highlight the tax implications of that. Do we need to formalise the sale\n"
     ]
    }
   ],
   "source": [
    "text = read_file(file_path)\n",
    "print(text[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_segmentation(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into smaller, manageable chunks (e.g., paragraphs).\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text to be segmented.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of segmented text chunks.\n",
    "    \"\"\"\n",
    "    # Split text by double newlines or similar paragraph markers\n",
    "    segments = re.split(r'\\n\\s*\\n', text.strip())\n",
    "    \n",
    "    return [segment.strip() for segment in segments if segment.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the text into words.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of tokens (words).\n",
    "    \"\"\"\n",
    "    # simple regex to split words\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up non-informative special characters or artifacts.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to remove special characters.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with unnecessary special characters removed.\n",
    "    \"\"\"\n",
    "    # Remove characters not usually found in legal texts\n",
    "    cleaned_text = re.sub(r'[^\\w\\s,.!?;:()-]', '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_structure(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Maintain the document's structural elements, such as headings.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process for structural preservation.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with preserved structure for headings and sections.\n",
    "    \"\"\"\n",
    "    # Keep lines starting with capital words as headings\n",
    "    structured_text = re.sub(r'(?m)^(?=[A-Z])(.+)$', r'## \\1', text)\n",
    "    \n",
    "    return structured_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text: str, tokenizer, chunk_size=4096) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates chunks of text for preprocessing, ensuring each chunk is within the specified size.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text to be chunked.\n",
    "        tokenizer (object): The tokenizer object to encode and decode text.\n",
    "        chunk_size (int): The desired size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=chunk_size)\n",
    "\n",
    "    # Split tokens into chunks of the specified size\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "    # Decode each chunk back into text\n",
    "    text_chunks = [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(file_path: str, tokenizer, chunk_size=4096, overlap=0) -> Tuple[str, List[str], str, List[str], str, List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess a legal document by executing a series of text processing steps, including chunking.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the legal document text file.\n",
    "        tokenizer (object): The tokenizer object to encode and decode text.\n",
    "        chunk_size (int): The desired size of each chunk.\n",
    "        overlap (int): The number of tokens to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "            - Original text (str)\n",
    "            - Segmented text chunks (List[str])\n",
    "            - Cleaned text (str)\n",
    "            - Tokenized words (List[str])\n",
    "            - Structured text (str)\n",
    "            - Chunks (List[str])\n",
    "    \"\"\"\n",
    "    # Load text from the file\n",
    "    text = read_file(file_path)\n",
    "\n",
    "    # Split text into segments (paragraphs)\n",
    "    segments = text_segmentation(text)\n",
    "\n",
    "    # Clean up non-informative special characters\n",
    "    cleaned_text = clean_special_characters(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "\n",
    "    # Preserve structural elements, e.g., headings\n",
    "    structured_text = preserve_structure(cleaned_text)\n",
    "\n",
    "    # Create chunks\n",
    "    chunks = create_chunks(cleaned_text, tokenizer, chunk_size)\n",
    "\n",
    "    return text, segments, cleaned_text, tokens, structured_text, chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ebc3e3aa3f43c09f2e9e8e3d7ac625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dcbae79b97401188516b77240293ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ed214a13394fbaaa5b25293fbe439c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b3035d944441fb9ee48ba8dd77dea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Input:\n",
      "I would like to seek expert advice regardin ...\n",
      "---------------------------------------------------------------------------\n",
      "Segments: [\"Input:\\nI would like to seek expert advice regarding our company's upcoming transaction involving the disposal of cryptocurrency assets. Specifically, we are planning to sell a substantial amount of Ethereum (ETH) and convert it into stablecoins due to market volatility. Please highlight the tax implications of that. Do we need to formalise the sale of ETH with a formal agreement?\\nFollow up question: What is the volume of the transactions?\\nFollow up question: What is the date of the transactions\\nFollow up question: What was the price of purchased ETH?/ What was the value of ETH expressed in stablecoin?\\nFollow up question: What is the stablecoin pegged to? \\nFollow up question: Has this transaction will happen between dependent or independent parties?\\nFollow up question: When is the transaction planned for?\\nFollow up question: Is trading crypto the company’s primary activity?\\nInput: 300 ETH converted to Tether, which is pegged to USD, transaction needs to go through by 1 August 2024, independent parties. Trading crypto is the company’s primary activity.\\nOutput:\\nIntroduction\\nLaw Firm X has been engaged by the company AB to comment on the tax implications of the disposal of ETH under the UK law and is planned for 1 August 2024. The customer wants to dispose of 300 ETH by 1 August 2024.\\nFactual state\\nAlpha Beta Ltd (“AB”) wants to dispose of 300 ETH in exchange for Tether. This memorandum provides an overview of the applicable laws and guidance which can have impact on the transactions and AB should be aware of.\\nAnalysis\\nWhilst no specific tax legislation is in place regarding the taxation of cryptocurrencies, His Majesty’s Revenue and Customs (HMRC) issued Cryptoassets Manual (CM) on 30 March 2021, which provides guidance on how HMRC is going to treat a transaction by corporation or individuals in relation to cryptoassets. CM is not legally binding on taxpayers, however it indicates the position likely to be taken by the HMRC concerning the crypto assets and therefore compliance to its contents is recommended.\\nAs per CM, section CRYPTO41250:\\n“Companies need to calculate their gain or loss when they dispose of their tokens to find out whether they need to pay Corporation Tax. A ‘disposal’ is a broad concept and includes:\\n(a) selling tokens for money\\n(b) exchanging tokens for a different type of token\\n(c) using tokens to pay for goods or services\\n(d) giving away tokens to another person”\\nStablecoins are also considered tokens. As per CRYPTO10100, \\n“Stablecoins are another prominent type of cryptoasset. The premise is that these tokens minimise volatility as they may be pegged to something that is considered to have a stable value such as a fiat currency (government-backed, for example US dollars) or precious metals such as gold.”).\\nTaking into account the fact that Tether is a token, the aforementioned scenario constitutes point (b) of the quoted passage. \\nAs being the party that is disposing of the ETH, AB needs to determine if they will be recording a loss or gain on their books, which is dependent on the price that is determined as the acquisition price of the ETH (Fair Market Value expressed in Tether).\\nDetermining the gain/loss\\nAs mentioned above, an important part to consider is also whether the party disposing of assets is whether they have made a gain or a loss and what will be recorded on disposing party’s books.\\nThis will be determined once the transaction price is known by 1 August 2024. \\nAs the AB’s primary activity is trading cryptocurrency, the loss shall be recognised as an operating loss.\\nWritten contract of disposal\\nIn order to address the question whether there needs to be a written contract prepared to comply with UK law, we need to first understand what can be understood as a contract in a crypto transaction.\\nCM provides guidance within CRYPTO41260. The section says that:\"]\n",
      "---------------------------------------------------------------------------\n",
      "Cleaned Text: Input:\n",
      "I would like to seek expert advice regardin ...\n",
      "---------------------------------------------------------------------------\n",
      "Tokens: ['Input', 'I', 'would', 'like', 'to']\n",
      "---------------------------------------------------------------------------\n",
      "Structured Text: ## Input:\n",
      "## I would like to seek expert advice re ...\n"
     ]
    }
   ],
   "source": [
    "# Using the preprocessor\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "# Initialise the Longformer tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# Call the preprocess_doc function\n",
    "original_text, segments, cleaned_text, tokens, structured_text, chunks = preprocess_doc(file_path, tokenizer)\n",
    "\n",
    "print(\"Original Text:\", original_text[:50], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Segments:\", segments[:1])\n",
    "print(\"---\" * 25)\n",
    "print(\"Cleaned Text:\", cleaned_text[:50], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Tokens:\", tokens[:5])\n",
    "print(\"---\" * 25)\n",
    "print(\"Structured Text:\", structured_text[:50], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 1517\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the cleaned_text to inspect the embedding size\n",
    "sample_tokenized = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "\n",
    "# Print the size of the embedding\n",
    "embedding_size = sample_tokenized['input_ids'].size(1)  # Get the number of tokens\n",
    "print(f\"Embedding size: {embedding_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai embedding function\n",
    "def get_embedding(docs: list[str]) -> list[list[float]]:\n",
    "    res = client.embeddings.create(\n",
    "        input=docs,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    \n",
    "    doc_embeds = [r.embedding for r in res.data]\n",
    "    \n",
    "    return doc_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the segmented text chunks\n",
    "chunk_embeddings = get_embedding(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, index_name: str, pinecone_api_key: str, openai_api_key: str, embedding_model: str=\"text-embedding-ada-002\"):\n",
    "        self.index_name = index_name\n",
    "        self.embedding_model = embedding_model\n",
    "\n",
    "        # Initialize Pinecone and OpenAI clients\n",
    "        self.pinecone_client = self.initialize_pinecone(pinecone_api_key)\n",
    "        self.openai_client = self.initialize_openai(openai_api_key)\n",
    "        self.index = self.pinecone_client.Index(self.index_name)\n",
    "\n",
    "    def initialize_pinecone(self, api_key: str):\n",
    "        # Create a Pinecone client instance\n",
    "        pc = Pinecone(api_key=api_key)\n",
    "\n",
    "        # Check if the index exists; if not, create it\n",
    "        if self.index_name not in pc.list_indexes().names():\n",
    "            pc.create_index(\n",
    "                name=self.index_name,\n",
    "                dimension=1536,  # Ensure this matches the output size of the embedding model\n",
    "                metric='cosine',\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud='aws',\n",
    "                    region='us-east-1'\n",
    "                )\n",
    "            )\n",
    "        return pc\n",
    "\n",
    "    def initialize_openai(self, api_key: str):\n",
    "        # Initialise OpenAI client\n",
    "        openai = OpenAI(\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        return openai\n",
    "\n",
    "    def get_index(self):\n",
    "        # Retrieve or create the index from Pinecone\n",
    "        return self.pinecone_client.Index(self.index_name)\n",
    "    \n",
    "    def get_embedding(self, text: str):\n",
    "        # Use OpenAI API to get text embeddings\n",
    "        response = self.openai_client.embeddings.create(input=text, model=self.embedding_model)\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    def upsert_vectors(self, vectors: list):\n",
    "        # Upsert vectors using the Pinecone index\n",
    "        try:\n",
    "            index = self.get_index()\n",
    "            index.upsert(vectors=vectors)\n",
    "            print(f\"Successfully upserted {len(vectors)} vectors to index {self.index_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Upsert Error: {e}\")\n",
    "\n",
    "    def query_vectors(self, query_vector: list, top_k: int = 3):\n",
    "        # Query the Pinecone index\n",
    "        index = self.get_index()\n",
    "        result = index.query(vector=query_vector, top_k=top_k)\n",
    "        print(\"Query results:\", result)  # Debugging print\n",
    "        return result['matches']\n",
    "\n",
    "    def retrieve_documents(self, question: str, top_k: int = 3):\n",
    "        # Retrieve documents relevant to the question\n",
    "        query_embedding = self.get_embedding(question)\n",
    "        matches = self.query_vectors(query_embedding, top_k=top_k)\n",
    "        print(\"Query matches:\", matches)  # Debugging print\n",
    "        # Check the actual keys available in matches to adjust retrieval\n",
    "        return [match.metadata.text for match in matches if 'metadata' in match and 'text' in match.metadata]\n",
    "\n",
    "    def generate_answer(self, question: str, context: str, model: str='gpt-4o-mini'):\n",
    "        # Generate an answer using OpenAI API\n",
    "        prompt = f\"\"\"You are a lawyer that helps retrieve knowledge from past\n",
    "                     memos and documents to answer subsequent questions.\n",
    "                     If the answer cannot be found, write \"I don't know.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\"\"\"\n",
    "\n",
    "        response = self.openai_client.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            max_tokens=50,\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "    def answer_question(self, question: str, top_k: int=3):\n",
    "        # Provide an answer to the question\n",
    "        documents = self.retrieve_documents(question, top_k)\n",
    "        context = \"\\n\".join(documents)\n",
    "        if not context:\n",
    "            return \"I don't know.\"\n",
    "        return self.generate_answer(question, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully upserted 1 vectors to index luthor-test-nb-0.\n",
      "Query results: {'matches': [{'id': 'doc_0', 'score': 0.75419426, 'values': []},\n",
      "             {'id': 'doc0', 'score': 0.00510411849, 'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 5}}\n",
      "Query matches: [{'id': 'doc_0', 'score': 0.75419426, 'values': []}, {'id': 'doc0', 'score': 0.00510411849, 'values': []}]\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Define the index name and API keys\n",
    "index_name = 'luthor-test-nb-0'\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Assuming the following variables are defined:\n",
    "# - file_path: path to the document file\n",
    "# - tokenizer: Longformer tokenizer\n",
    "# - preprocess_doc: function that preprocesses the document\n",
    "original_text, segments, cleaned_text, tokens, structured_text, chunks = preprocess_doc(file_path, tokenizer)\n",
    "\n",
    "# Initialize the RAG system\n",
    "rag_system = RAGSystem(index_name, pinecone_api_key, openai_api_key)\n",
    "\n",
    "# Generate embeddings for the segmented text chunks\n",
    "chunk_embeddings = [rag_system.get_embedding(chunk) for chunk in chunks]\n",
    "\n",
    "# Create vector dictionaries and upsert documents\n",
    "vectors = [\n",
    "    {\"id\": f\"doc_{i}\", \"values\": chunk_embeddings[i], \"metadata\": {\"text\": chunks[i]}}\n",
    "    for i in range(len(chunks))\n",
    "]\n",
    "\n",
    "rag_system.upsert_vectors(vectors)\n",
    "\n",
    "# Answer a question using the RAG system\n",
    "question = \"What are the essential elements of a contract in English law?\"\n",
    "answer = rag_system.answer_question(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
