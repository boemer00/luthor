{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/renatoboemer/code/developer/luthor/data/Memo 2  - Crypto assets disposal - FINISHED .docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from docx import Document\n",
    "\n",
    "def read_file(file_path):\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    # Get the file extension\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    # Handle .txt files\n",
    "    if file_extension.lower() == '.txt':\n",
    "        return read_txt(file_path)\n",
    "\n",
    "    # Handle .docx files\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        return read_docx(file_path)\n",
    "\n",
    "    # Handle .pdf files\n",
    "    elif file_extension.lower() == '.pdf':\n",
    "        return read_pdf(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_docx(file_path):\n",
    "    document = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in document.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    document = fitz.open(file_path)\n",
    "    all_text = []\n",
    "    for page in document:\n",
    "        text = page.get_text()\n",
    "        all_text.append(text)\n",
    "    return '\\n'.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_file(file_path)\n",
    "print(text[:350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_segmentation(text):\n",
    "    \"\"\"\n",
    "    Split text into smaller, manageable chunks (e.g., paragraphs).\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text to be segmented.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of segmented text chunks.\n",
    "    \"\"\"\n",
    "    # Split text by double newlines or similar paragraph markers\n",
    "    segments = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# # Load the tokenizer and model for NER\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# # Create a pipeline for NER\n",
    "# ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# # Example text for NER\n",
    "# example = \"Hugging Face was founded in 2016 by French entrepreneurs Cl√©ment Delangue, Julien Chaumond, and Thomas Wolf in New York City.\"\n",
    "\n",
    "# # Run the NER pipeline on the example text\n",
    "# ner_results = ner_pipeline(example)\n",
    "# print(ner_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organized_results = {'LOC': [], 'PER': [], 'ORG': [], 'MISC': []}\n",
    "\n",
    "# current_entity = None\n",
    "# current_words = []\n",
    "\n",
    "# for result in ner_results:\n",
    "#     entity_type = result['entity'].split('-')[1]\n",
    "#     if result['entity'].startswith('B-'):\n",
    "#         if current_entity:\n",
    "#             organized_results[current_entity].append(' '.join(current_words))\n",
    "#         current_entity = entity_type\n",
    "#         current_words = [result['word']]\n",
    "#     elif result['entity'].startswith('I-') and current_entity == entity_type:\n",
    "#         current_words.append(result['word'])\n",
    "\n",
    "# # Handle the last entity\n",
    "# if current_entity:\n",
    "#     organized_results[current_entity].append(' '.join(current_words))\n",
    "\n",
    "# # Remove hash symbols from words\n",
    "# for key, value in organized_results.items():\n",
    "#     organized_results[key] = [' '.join(word.split('##')) for word in value]\n",
    "\n",
    "# print(organized_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text into words.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of tokens (words).\n",
    "    \"\"\"\n",
    "    # Use simple regex to split words; nlp(text) can be used for more advanced tokenization\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(text):\n",
    "    \"\"\"\n",
    "    Clean up non-informative special characters or artifacts.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to remove special characters.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with unnecessary special characters removed.\n",
    "    \"\"\"\n",
    "    # Remove characters not usually found in legal texts\n",
    "    cleaned_text = re.sub(r'[^\\w\\s,.!?;:()-]', '', text)\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_structure(text):\n",
    "    \"\"\"\n",
    "    Maintain the document's structural elements, such as headings.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process for structural preservation.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with preserved structure for headings and sections.\n",
    "    \"\"\"\n",
    "    # This can involve wrapping or tagging headings, using markdown for sections\n",
    "    structured_text = text\n",
    "    \n",
    "    # Example: Keep lines starting with capital words as headings\n",
    "    structured_text = re.sub(r'(?m)^(?=[A-Z])(.+)$', r'## \\1', structured_text)\n",
    "    \n",
    "    return structured_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(file_path):\n",
    "    \"\"\"\n",
    "    Preprocess a legal document by executing a series of text processing steps.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the legal document text file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "            - Original text (str)\n",
    "            - Segmented text chunks (List[str])\n",
    "            - Cleaned text (str)\n",
    "            - Tokenized words (List[str])\n",
    "            - Structured text (str)\n",
    "    \"\"\"\n",
    "    # Load text from the file\n",
    "    text = read_file(file_path)\n",
    "\n",
    "    # Split text into segments (paragraphs)\n",
    "    segments = text_segmentation(text)\n",
    "\n",
    "    # Clean up non-informative special characters\n",
    "    cleaned_text = clean_special_characters(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    # Preserve structural elements, e.g., headings\n",
    "    structured_text = preserve_structure(text)\n",
    "\n",
    "    return text, segments, cleaned_text, tokens, structured_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "original_text, segments, cleaned_text, tokens, structured_text = preprocess_doc(file_path)\n",
    "\n",
    "print(\"Original Text:\", original_text[:200], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Segments:\", segments[:3])\n",
    "print(\"---\" * 25)\n",
    "print(\"Cleaned Text:\", cleaned_text[:200], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Tokens:\", tokens[:10])\n",
    "print(\"---\" * 25)\n",
    "print(\"Structured Text:\", structured_text[:200], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_text_chunks(chunks, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Generate embeddings for text chunks using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): The list of text chunks to embed.\n",
    "        model_name (str): The pre-trained model name from sentence-transformers.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "            - Text chunks (List[str])\n",
    "            - Corresponding embeddings (List[List[float]])\n",
    "    \"\"\"\n",
    "    # Load the embedding model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Generate embeddings for each chunk\n",
    "    embeddings = model.encode(chunks, convert_to_tensor=False).tolist()\n",
    "    \n",
    "    return chunks, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text, segments, cleaned_text, tokens, structured_text = preprocess_doc(file_path)\n",
    "\n",
    "# Generate embeddings for the segmented text chunks\n",
    "chunks, embeddings = embed_text_chunks(segments)\n",
    "\n",
    "# Display some of the embeddings\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(chunk)\n",
    "    print(f\"Embedding (first 5 values): {embedding[:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
