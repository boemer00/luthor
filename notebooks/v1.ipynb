{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/renatoboemer/code/developer/luthor/data/Memo 2  - Crypto assets disposal - FINISHED .docx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "from docx import Document\n",
    "\n",
    "def read_file(file_path):\n",
    "    # Check if file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "    # Get the file extension\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    # Handle .txt files\n",
    "    if file_extension.lower() == '.txt':\n",
    "        return read_txt(file_path)\n",
    "\n",
    "    # Handle .docx files\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        return read_docx(file_path)\n",
    "\n",
    "    # Handle .pdf files\n",
    "    elif file_extension.lower() == '.pdf':\n",
    "        return read_pdf(file_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def read_docx(file_path):\n",
    "    document = Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in document.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    document = fitz.open(file_path)\n",
    "    all_text = []\n",
    "    for page in document:\n",
    "        text = page.get_text()\n",
    "        all_text.append(text)\n",
    "    return '\\n'.join(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "I would like to seek expert advice regarding our company's upcoming transaction involving the\n"
     ]
    }
   ],
   "source": [
    "text = read_file(file_path)\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_segmentation(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into smaller, manageable chunks with consideration for legal memo structures.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text to be segmented.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of segmented text chunks.\n",
    "    \"\"\"\n",
    "    # Identify section breaks (e.g. double newlines, headings, and bullet points).\n",
    "    pattern = r'(?<=\\n)(?=\\n)|(?<=\\n)(?=\\s*[\\d-]+\\s)|(?<=\\n)(?=Section \\d+|Article \\d+)|(?<=\\n)(?=\\s*-\\s)|(?<=\\n)(?=\\s*\\*\\s)'\n",
    "\n",
    "    # Split the text using the defined pattern\n",
    "    segments = re.split(pattern, text.strip())\n",
    "\n",
    "    # Clean up the segments to remove any leading/trailing whitespace\n",
    "    segmented_text = [segment.strip() for segment in segments if segment.strip()]\n",
    "\n",
    "    return segmented_text\n",
    "\n",
    "\n",
    "# segmented_text = text_segmentation(text)\n",
    "# segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure nltk resources are downloaded during setup or first run\n",
    "def setup_nltk():\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "setup_nltk()\n",
    "\n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize the text into words, considering legal-specific tokens and preprocessing.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of tokens (words).\n",
    "    \"\"\"\n",
    "    # # Remove unwanted characters, retain alphanumeric and some punctuation relevant to legal text\n",
    "    # cleaned_text = re.sub(r'[^\\w\\s.,;:()\\'\\\"-]', ' ', text)\n",
    "\n",
    "    # Tokenize using NLTK's word tokenizer, which handles punctuation better than simple regex\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase to maintain consistency\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "\n",
    "    # Remove stopwords specific to legal context if needed\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize tokens to reduce them to their base form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean up non-informative special characters or artifacts.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text from which to remove special characters.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text with unnecessary special characters removed.\n",
    "    \"\"\"\n",
    "    # Remove characters not usually found in legal texts\n",
    "    cleaned_text = re.sub(r'[^\\w\\s,.!?;:()-]', '', text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_structure(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Maintain the document's structural elements, such as headings.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process for structural preservation.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with preserved structure for headings and sections.\n",
    "    \"\"\"\n",
    "    # Keep lines starting with capital words as headings\n",
    "    structured_text = re.sub(r'(?m)^(?=[A-Z])(.+)$', r'## \\1', text)\n",
    "\n",
    "    return structured_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text: str, tokenizer, chunk_size=4096) -> List[str]:\n",
    "    \"\"\"\n",
    "    Creates chunks of text for preprocessing, ensuring each chunk is within the specified size.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full text to be chunked.\n",
    "        tokenizer (object): The tokenizer object to encode and decode text.\n",
    "        chunk_size (int): The desired size of each chunk.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=chunk_size)\n",
    "\n",
    "    # Split tokens into chunks of the specified size\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "    # Decode each chunk back into text\n",
    "    text_chunks = [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
    "\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(file_path: str, tokenizer, chunk_size=4096, overlap=0) -> Tuple[str, List[str], str, List[str], str, List[str]]:\n",
    "    \"\"\"\n",
    "    Preprocess a legal document by executing a series of text processing steps, including chunking.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the legal document text file.\n",
    "        tokenizer (object): The tokenizer object to encode and decode text.\n",
    "        chunk_size (int): The desired size of each chunk.\n",
    "        overlap (int): The number of tokens to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "            - Original text (str)\n",
    "            - Segmented text chunks (List[str])\n",
    "            - Cleaned text (str)\n",
    "            - Tokenized words (List[str])\n",
    "            - Structured text (str)\n",
    "            - Chunks (List[str])\n",
    "    \"\"\"\n",
    "    # Load text from the file\n",
    "    text = read_file(file_path)\n",
    "\n",
    "    # Split text into segments (paragraphs)\n",
    "    segments = text_segmentation(text)\n",
    "\n",
    "    # Clean up non-informative special characters\n",
    "    cleaned_text = clean_special_characters(text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    tokens = tokenize_text(cleaned_text)\n",
    "\n",
    "    # Preserve structural elements (headings)\n",
    "    structured_text = preserve_structure(cleaned_text)\n",
    "\n",
    "    # Create chunks\n",
    "    chunks = create_chunks(cleaned_text, tokenizer, chunk_size)\n",
    "\n",
    "    return text, segments, cleaned_text, tokens, structured_text, chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Input:\n",
      "I would like to seek expert advice regardin ...\n",
      "---------------------------------------------------------------------------\n",
      "Segments: [\"Input:\\nI would like to seek expert advice regarding our company's upcoming transaction involving the disposal of cryptocurrency assets. Specifically, we are planning to sell a substantial amount of Ethereum (ETH) and convert it into stablecoins due to market volatility. Please highlight the tax implications of that. Do we need to formalise the sale of ETH with a formal agreement?\\nFollow up question: What is the volume of the transactions?\\nFollow up question: What is the date of the transactions\\nFollow up question: What was the price of purchased ETH?/ What was the value of ETH expressed in stablecoin?\\nFollow up question: What is the stablecoin pegged to? \\nFollow up question: Has this transaction will happen between dependent or independent parties?\\nFollow up question: When is the transaction planned for?\\nFollow up question: Is trading crypto the company’s primary activity?\\nInput: 300 ETH converted to Tether, which is pegged to USD, transaction needs to go through by 1 August 2024, independent parties. Trading crypto is the company’s primary activity.\\nOutput:\\nIntroduction\\nLaw Firm X has been engaged by the company AB to comment on the tax implications of the disposal of ETH under the UK law and is planned for 1 August 2024. The customer wants to dispose of 300 ETH by 1 August 2024.\\nFactual state\\nAlpha Beta Ltd (“AB”) wants to dispose of 300 ETH in exchange for Tether. This memorandum provides an overview of the applicable laws and guidance which can have impact on the transactions and AB should be aware of.\\nAnalysis\\nWhilst no specific tax legislation is in place regarding the taxation of cryptocurrencies, His Majesty’s Revenue and Customs (HMRC) issued Cryptoassets Manual (CM) on 30 March 2021, which provides guidance on how HMRC is going to treat a transaction by corporation or individuals in relation to cryptoassets. CM is not legally binding on taxpayers, however it indicates the position likely to be taken by the HMRC concerning the crypto assets and therefore compliance to its contents is recommended.\\nAs per CM, section CRYPTO41250:\\n“Companies need to calculate their gain or loss when they dispose of their tokens to find out whether they need to pay Corporation Tax. A ‘disposal’ is a broad concept and includes:\\n(a) selling tokens for money\\n(b) exchanging tokens for a different type of token\\n(c) using tokens to pay for goods or services\\n(d) giving away tokens to another person”\\nStablecoins are also considered tokens. As per CRYPTO10100, \\n“Stablecoins are another prominent type of cryptoasset. The premise is that these tokens minimise volatility as they may be pegged to something that is considered to have a stable value such as a fiat currency (government-backed, for example US dollars) or precious metals such as gold.”).\\nTaking into account the fact that Tether is a token, the aforementioned scenario constitutes point (b) of the quoted passage. \\nAs being the party that is disposing of the ETH, AB needs to determine if they will be recording a loss or gain on their books, which is dependent on the price that is determined as the acquisition price of the ETH (Fair Market Value expressed in Tether).\\nDetermining the gain/loss\\nAs mentioned above, an important part to consider is also whether the party disposing of assets is whether they have made a gain or a loss and what will be recorded on disposing party’s books.\\nThis will be determined once the transaction price is known by 1 August 2024. \\nAs the AB’s primary activity is trading cryptocurrency, the loss shall be recognised as an operating loss.\\nWritten contract of disposal\\nIn order to address the question whether there needs to be a written contract prepared to comply with UK law, we need to first understand what can be understood as a contract in a crypto transaction.\\nCM provides guidance within CRYPTO41260. The section says that:\"]\n",
      "---------------------------------------------------------------------------\n",
      "Cleaned Text: Input:\n",
      "I would like to seek expert advice regardin ...\n",
      "---------------------------------------------------------------------------\n",
      "Tokens: ['input', ':', 'would', 'like', 'seek']\n",
      "---------------------------------------------------------------------------\n",
      "Structured Text: ## Input:\n",
      "## I would like to seek expert advice re ...\n",
      "---------------------------------------------------------------------------\n",
      "Chunks: ['Input:\\nI would like to seek expert advice regarding our companys upcoming transaction involving the disposal of cryptocurrency assets. Specifically, we are planning to sell a substantial amount of Ethereum (ETH) and convert it into stablecoins due to market volatility. Please highlight the tax implications of that. Do we need to formalise the sale of ETH with a formal agreement?\\nFollow up question: What is the volume of the transactions?\\nFollow up question: What is the date of the transactions\\nFollow up question: What was the price of purchased ETH? What was the value of ETH expressed in stablecoin?\\nFollow up question: What is the stablecoin pegged to? \\nFollow up question: Has this transaction will happen between dependent or independent parties?\\nFollow up question: When is the transaction planned for?\\nFollow up question: Is trading crypto the companys primary activity?\\nInput: 300 ETH converted to Tether, which is pegged to USD, transaction needs to go through by 1 August 2024, independent parties. Trading crypto is the companys primary activity.\\nOutput:\\nIntroduction\\nLaw Firm X has been engaged by the company AB to comment on the tax implications of the disposal of ETH under the UK law and is planned for 1 August 2024. The customer wants to dispose of 300 ETH by 1 August 2024.\\nFactual state\\nAlpha Beta Ltd (AB) wants to dispose of 300 ETH in exchange for Tether. This memorandum provides an overview of the applicable laws and guidance which can have impact on the transactions and AB should be aware of.\\nAnalysis\\nWhilst no specific tax legislation is in place regarding the taxation of cryptocurrencies, His Majestys Revenue and Customs (HMRC) issued Cryptoassets Manual (CM) on 30 March 2021, which provides guidance on how HMRC is going to treat a transaction by corporation or individuals in relation to cryptoassets. CM is not legally binding on taxpayers, however it indicates the position likely to be taken by the HMRC concerning the crypto assets and therefore compliance to its contents is recommended.\\nAs per CM, section CRYPTO41250:\\nCompanies need to calculate their gain or loss when they dispose of their tokens to find out whether they need to pay Corporation Tax. A disposal is a broad concept and includes:\\n(a) selling tokens for money\\n(b) exchanging tokens for a different type of token\\n(c) using tokens to pay for goods or services\\n(d) giving away tokens to another person\\nStablecoins are also considered tokens. As per CRYPTO10100, \\nStablecoins are another prominent type of cryptoasset. The premise is that these tokens minimise volatility as they may be pegged to something that is considered to have a stable value such as a fiat currency (government-backed, for example US dollars) or precious metals such as gold.).\\nTaking into account the fact that Tether is a token, the aforementioned scenario constitutes point (b) of the quoted passage. \\nAs being the party that is disposing of the ETH, AB needs to determine if they will be recording a loss or gain on their books, which is dependent on the price that is determined as the acquisition price of the ETH (Fair Market Value expressed in Tether).\\nDetermining the gainloss\\nAs mentioned above, an important part to consider is also whether the party disposing of assets is whether they have made a gain or a loss and what will be recorded on disposing partys books.\\nThis will be determined once the transaction price is known by 1 August 2024. \\nAs the ABs primary activity is trading cryptocurrency, the loss shall be recognised as an operating loss.\\nWritten contract of disposal\\nIn order to address the question whether there needs to be a written contract prepared to comply with UK law, we need to first understand what can be understood as a contract in a crypto transaction.\\nCM provides guidance within CRYPTO41260. The section says that: \\n\\nTokens cannot simply be transferred from the distributed ledger for one cryptoasset to the distributed ledger for a different cryptoasset. For example, bitcoin cannot exist on the Ethereum blockchain. An effect comparable to a swap can be achieved using a smart contract and secure public address. The holder of the tokens uses a smart contract to transfer tokens to a public address that they dont control. An equivalent amount of tokens of the second cryptoasset are transferred from a secure public address to a public address controlled by the person.\\nGiven the fact pattern that the client wants to swap ETH for Tether, this can be done using a smart contract. \\nThis is permitted by a document called Legal statement on cryptoassets and smart contracts (Legal Statement) published by UK Jurisdiction Taskforce (UKJT) which allows traditional contracts to be replaced by smart contracts if it is enabled by the protocol. Although Legal Statement is not formally a part of UK Law, it is providing a foundation for the responsible future utilisation of cryptoassets and smart contracts (as per Foreword of the Legal Statement prepared by Sir Geoffrey Vos, Chancellor of the High Court). \\nSections 18 and 19 of the Legal Statement describe the weight of a smart contract and make a comparison to traditional contract.\\n18. There is a contract in English law when two or more parties have reached an agreement, intend to create a legal relationship by doing so, and have each given something of benefit. A smart contract is capable of satisfying those requirements just as well as a more traditional or natural language contract, and a smart contract is therefore capable of having contractual force. Whether the requirements are in fact met in any given case will depend on the parties words and conduct, just as it does with any other contract.\\n19. The parties contractual obligations may be defined by computer code (in which case there may be little room for interpretation in the traditional sense) or the code may merely implement an agreement whose meaning is to be found elsewhere (in which case the code is unimportant from the perspective of defining the agreement). Either way, however, in principle a smart contract can be identified, interpreted and enforced using ordinary and well-established legal principles.\\nGiven the above it can be concluded that since the terms of the transaction are defined and confirmed  in the smart contract, there is no need for an additional traditional contract between the parties.\\nOne-way transfers\\nIt is also important to note that some transfers can only go in one direction, meaning that once the transfer has been made it cannot be undone or transferred back at a future date.\\nCM describes in CRYPTO41260 an example of such situation in case of ETH transactions: \\nAn example of this can be seen with the Ethereum blockchain. Currently ether are on the Ethereum mainnet (short for main network, the main public Ethereum blockchain). Holders of ether can choose to transfer their tokens from the mainnet to a different blockchain called the Beacon Chain. The Beacon Chain blockchain is where Ethereums Proof of Stake will be implemented (for more information on Proof of Stake see CRYPTO10300, manualcryptoassets-manualcrypto10300). It will be impossible to transfer ether from the Beacon Chain to the mainnet, making transfers a one-way process only.\\nGiven that the client is considering a disposal of ETH, it is something worth remembering that given the technical capabilities of the network, the transaction cannot be reversed.\\n\\n\\n\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "# Using the preprocessor\n",
    "from transformers import LongformerTokenizer\n",
    "\n",
    "# Initialise the Longformer tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
    "\n",
    "# Call the preprocess_doc function\n",
    "original_text, segments, cleaned_text, tokens, structured_text, chunks = preprocess_doc(file_path, tokenizer)\n",
    "\n",
    "print(\"Original Text:\", original_text[:50], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Segments:\", segments[:1])\n",
    "print(\"---\" * 25)\n",
    "print(\"Cleaned Text:\", cleaned_text[:50], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Tokens:\", tokens[:5])\n",
    "print(\"---\" * 25)\n",
    "print(\"Structured Text:\", structured_text[:50], \"...\")\n",
    "print(\"---\" * 25)\n",
    "print(\"Chunks:\", chunks[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size: 1517\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the cleaned_text to inspect the embedding size\n",
    "sample_tokenized = tokenizer(cleaned_text, return_tensors=\"pt\", max_length=4096, truncation=True)\n",
    "\n",
    "# Print the size of the embedding\n",
    "embedding_size = sample_tokenized['input_ids'].size(1)  # Get the number of tokens\n",
    "print(f\"Embedding size: {embedding_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone client initialized: True\n",
      "OpenAI client initialized: True\n",
      "Index 'luthor-test-nb-0' exists or created: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import openai\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "index_name = 'luthor-test-nb-0'\n",
    "pinecone_client = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Check if the index exists; if not, create it\n",
    "if index_name not in pinecone_client.list_indexes().names():\n",
    "    pinecone_client.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  # Ensure this matches the output size of the embedding model\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "# Retrieve or create the index from Pinecone\n",
    "index = pinecone_client.Index(index_name)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "print(f\"Pinecone client initialized: {pinecone_client is not None}\")\n",
    "print(f\"OpenAI client initialized: {openai.api_key is not None}\")\n",
    "print(f\"Index '{index_name}' exists or created: {index_name in pinecone_client.list_indexes().names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 0: [0.03321181610226631]...\n",
      "Number of embeddings: 1\n",
      "First embedding (first 5 dimensions): [0.03321181610226631, 0.00520020117983222, 0.02285817265510559, 0.01826249249279499, -0.0008028248557820916]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text: str, model: str = \"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(input=text, model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Assuming the chunks are obtained from the preprocessing step\n",
    "chunk_embeddings = [get_embedding(chunk) for chunk in chunks]\n",
    "\n",
    "# Debug: Print some embeddings to verify\n",
    "for i, embedding in enumerate(chunk_embeddings):\n",
    "    print(f\"Embedding {i}: {embedding[:1]}...\")\n",
    "\n",
    "\n",
    "# Verify embeddings\n",
    "print(f\"Number of embeddings: {len(chunk_embeddings)}\")\n",
    "print(f\"First embedding (first 5 dimensions): {chunk_embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully upserted 1 vectors to index luthor-test-nb-0.\n"
     ]
    }
   ],
   "source": [
    "# Create vector dictionaries for upsertion\n",
    "vectors = [\n",
    "    {\n",
    "        \"id\": f\"doc_{i}\",\n",
    "        \"values\": chunk_embeddings[i],\n",
    "        \"metadata\": {\n",
    "            \"text\": chunks[i],\n",
    "            \"source\": \"database\"\n",
    "        }\n",
    "    }\n",
    "    for i in range(len(chunks))\n",
    "]\n",
    "\n",
    "\n",
    "# Upsert vectors into Pinecone index\n",
    "index.upsert(vectors=vectors)\n",
    "\n",
    "# Verify upsertion\n",
    "print(f\"Successfully upserted {len(vectors)} vectors to index {index_name}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 2\n",
      "Match 0: I...\n",
      "Match 1: I...\n"
     ]
    }
   ],
   "source": [
    "def query_pinecone(query_vector: list, top_k: int = 3):\n",
    "    # Perform a query on the Pinecone index\n",
    "    response = index.query(vector=query_vector, top_k=top_k, include_values=True, include_metadata=True)\n",
    "    matches = response['matches']\n",
    "\n",
    "    # Debugging print to verify structure\n",
    "    # print(f\"Query response structure: {matches}\")\n",
    "\n",
    "    return matches\n",
    "\n",
    "# Define a question and generate its embedding\n",
    "question = \"What are the essential elements of a contract in English law?\"\n",
    "query_embedding = get_embedding(question)\n",
    "\n",
    "# Query Pinecone index\n",
    "matches = query_pinecone(query_embedding)\n",
    "\n",
    "# Verify query results\n",
    "print(f\"Number of matches: {len(matches)}\")\n",
    "for i, match in enumerate(matches):\n",
    "    # Check if metadata exists and is accessible\n",
    "    if 'metadata' in match and 'text' in match['metadata']:\n",
    "        print(f\"Match {i}: {match['metadata']['text'][:1]}...\")  # Print the first character\n",
    "    else:\n",
    "        print(f\"Match {i} has no metadata or text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: In English law, the essential elements of a contract are as follows:\n",
      "\n",
      "1. **Offer**: One party must make a clear and definite offer to enter into an agreement.\n",
      "\n",
      "2. **Acceptance**: The other party must accept the offer in its exact terms. Acceptance must be communicated to the offeror.\n",
      "\n",
      "3. **Consideration**: There must be something of value exchanged between the parties. This can be money, services, goods, or a promise to do (or not do) something.\n",
      "\n",
      "4. **Intention to Create Legal Relations**: The parties must intend for the agreement to be legally binding. In commercial agreements, this intention is usually presumed.\n",
      "\n",
      "5. **Capacity**: The parties must have the legal capacity to enter into a contract. This generally means they must be of legal age (18 years or older in most cases) and of sound mind.\n",
      "\n",
      "6. **Legality**: The purpose of the contract must be lawful. Contracts that involve illegal activities are not enforceable.\n",
      "\n",
      "7. **Certainty**: The terms of the contract must be clear enough that the parties understand their obligations. If the terms are vague or uncertain, the contract may be unenforceable.\n",
      "\n",
      "These elements ensure that a contract is valid and\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def generate_answer(question: str, context: str, model: str = 'gpt-4o-mini'):\n",
    "    if not context.strip():\n",
    "        return \"I don't know.\"\n",
    "\n",
    "    # Identify context source\n",
    "    if \"database\" in context:\n",
    "        context_source = \"from the database\"\n",
    "    else:\n",
    "        context_source = \"from my general knowledge\"\n",
    "\n",
    "    prompt = f\"\"\"You are an experienced lawyer specializing in extracting and interpreting information from legal\n",
    "    documents, past memos, and other records to answer questions accurately. Your goal is to provide the most\n",
    "    reliable and detailed advice possible based on the available information.\n",
    "\n",
    "    Context (Source: {context_source}): {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Guidelines:\n",
    "    - Use the context provided to support your answer.\n",
    "    - If the answer is not available, suggest potential research avenues or considerations that may help.\n",
    "    - Structure your response to address the question clearly and logically.\n",
    "    - If unable to answer, state 'I don't know,' but also indicate why (e.g., insufficient context, unclear question).\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the message prompt for the chat completion\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an experienced lawyer specializing in extracting and interpreting information from legal documents to provide accurate advice.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {question}\"}\n",
    "    ]\n",
    "\n",
    "    # Call the chat completions API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=250,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's message from the response\n",
    "    generated_answer = response.choices[0].message.content.strip()\n",
    "    return generated_answer\n",
    "\n",
    "# Compile context from retrieved matches\n",
    "retrieved_texts = [match['metadata']['text'] for match in matches if 'metadata' in match and 'text' in match['metadata']]\n",
    "context = \"\\n\".join(retrieved_texts)\n",
    "\n",
    "# Generate an answer\n",
    "answer = generate_answer(question, context)\n",
    "\n",
    "# Verify generated answer\n",
    "print(f\"Generated Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_answer(answer: str, context: str):\n",
    "    if any(chunk in answer for chunk in context.split(\"\\n\")):\n",
    "        print(\"Answer derived from database context.\")\n",
    "    else:\n",
    "        print(\"Answer likely generated by the language model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone_with_score(query_vector: list, top_k: int=3):\n",
    "    response = index.query(vector=query_vector, top_k=top_k, include_values=True, include_metadata=True)\n",
    "    matches = response['matches']\n",
    "\n",
    "    # Determine if the highest score is above a certain threshold\n",
    "    if matches and matches[0]['score'] > 0.9:\n",
    "        print(\"High confidence in database-derived answer.\")\n",
    "    else:\n",
    "        print(\"Answer may be more LLM-derived due to lower confidence.\")\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer derived from database context.\n"
     ]
    }
   ],
   "source": [
    "# Analyse the answer\n",
    "analyze_answer(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer may be more LLM-derived due to lower confidence.\n"
     ]
    }
   ],
   "source": [
    "# Query Pinecone\n",
    "matches = query_pinecone_with_score(query_embedding)\n",
    "\n",
    "# Compile context from matches\n",
    "retrieved_texts = [match['metadata']['text'] for match in matches if 'metadata' in match and 'text' in match['metadata']]\n",
    "context = \"\\n\".join(retrieved_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
